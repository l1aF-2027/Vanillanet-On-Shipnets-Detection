@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}
@misc{chen2023vanillanetpowerminimalismdeep,
      title={VanillaNet: the Power of Minimalism in Deep Learning}, 
      author={Hanting Chen and Yunhe Wang and Jianyuan Guo and Dacheng Tao},
      year={2023},
      eprint={2305.12972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.12972}, 
}
@inproceedings{imagenet,
title = "ImageNet: A Large-Scale Hierarchical Image Database",
abstract = "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
author = "Jia Deng and Wei Dong and Richard Socher and Li, {Li Jia} and Kai Li and Li Fei-Fei",
note = "Publisher Copyright: {\textcopyright} 2009 IEEE.; 2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009 ; Conference date: 20-06-2009 Through 25-06-2009",
year = "2009",
doi = "10.1109/CVPR.2009.5206848",
language = "English (US)",
isbn = "9781424439911",
series = "2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009",
publisher = "IEEE Computer Society",
pages = "248--255",
booktitle = "2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009",
address = "United States",
}
@misc{chen2021universaladderneuralnetworks,
      title={Universal Adder Neural Networks}, 
      author={Hanting Chen and Yunhe Wang and Chang Xu and Chao Xu and Chunjing Xu and Tong Zhang},
      year={2021},
      eprint={2105.14202},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2105.14202}, 
}
@online{qochuk_im2col,
    author = {QoChuk, Benjamin and {OpenGenus team}},
    title = {im2col Convolution},
    organization = {OpenGenus},
    url = {iq.opengenus.org/im2col},
    website = {iq.opengenus.org}
}
@misc{robert_hammell_2018,
	title={Ships in Satellite Imagery},
	url={https://www.kaggle.com/dsv/61115},
	DOI={10.34740/KAGGLE/DSV/61115},
	publisher={Kaggle},
	author={Robert Hammell},
	year={2018}
}
@misc{ding2021diversebranchblockbuilding,
      title={Diverse Branch Block: Building a Convolution as an Inception-like Unit}, 
      author={Xiaohan Ding and Xiangyu Zhang and Jungong Han and Guiguang Ding},
      year={2021},
      eprint={2103.13425},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.13425}, 
}
@misc{ding2021repvggmakingvggstyleconvnets,
      title={RepVGG: Making VGG-style ConvNets Great Again}, 
      author={Xiaohan Ding and Xiangyu Zhang and Ningning Ma and Jungong Han and Guiguang Ding and Jian Sun},
      year={2021},
      eprint={2101.03697},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2101.03697}, 
}
@misc{ding2022scalingkernels31x31revisiting,
      title={Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs}, 
      author={Xiaohan Ding and Xiangyu Zhang and Yizhuang Zhou and Jungong Han and Guiguang Ding and Jian Sun},
      year={2022},
      eprint={2203.06717},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2203.06717}, 
}
@misc{ding2019acnetstrengtheningkernelskeletons,
      title={ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks}, 
      author={Xiaohan Ding and Yuchen Guo and Guiguang Ding and Jungong Han},
      year={2019},
      eprint={1908.03930},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.03930}, 
}
@misc{he2015delvingdeeprectifierssurpassing,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}
@misc{hendrycks2023gaussianerrorlinearunits,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}
@misc{ramachandran2017searchingactivationfunctions,
      title={Searching for Activation Functions}, 
      author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1710.05941},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1710.05941}, 
}
@misc{mhaskar2016deepvsshallownetworks,
      title={Deep vs. shallow networks : An approximation theory perspective}, 
      author={Hrushikesh Mhaskar and Tomaso Poggio},
      year={2016},
      eprint={1608.03287},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1608.03287}, 
}
@misc{eldan2016powerdepthfeedforwardneural,
      title={The Power of Depth for Feedforward Neural Networks}, 
      author={Ronen Eldan and Ohad Shamir},
      year={2016},
      eprint={1512.03965},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1512.03965}, 
}
@misc{telgarsky2016benefitsdepthneuralnetworks,
      title={Benefits of depth in neural networks}, 
      author={Matus Telgarsky},
      year={2016},
      eprint={1602.04485},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.04485}, 
}
@misc{xu2020batchnormalizationenhancedlinear,
      title={Batch Normalization with Enhanced Linear Transformation}, 
      author={Yuhui Xu and Lingxi Xie and Cihang Xie and Jieru Mei and Siyuan Qiao and Wei Shen and Hongkai Xiong and Alan Yuille},
      year={2020},
      eprint={2011.14150},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2011.14150}, 
}